server.port=8080

mybatis.configuration.default-enum-type-handler = org.apache.ibatis.type.EnumOrdinalTypeHandler
mybatis.configuration.map-underscore-to-camel-case=true
mybatis.type-aliases-package=com.example.model
mybatis.configuration.use-generated-keys = true
spring.datasource.url= jdbc:mysql://47.101.172.249:3301/campus_im?serverTimezone=GMT%2B8
spring.datasource.username=root
spring.datasource.password=123456kk2021
spring.datasource.hikari.driver-class-name=com.mysql.cj.jdbc.Driver

swagger.base-package=com.example.rest.controller
swagger.exclude-path=/error/**

spring.mail.host=smtp.qq.com
spring.mail.port=465
spring.mail.username=3025957737@qq.com
spring.mail.password=rbbmuhxzzcsadcfi
spring.mail.protocol=smtps
spring.mail.properties.mail.smtp.ssl.enable=true

spring.redis.host=47.101.172.249
spring.redis.port=3302
spring.redis.database=0
spring.redis.timeout=3s
spring.redis.ssl=false

# pagehelper
pagehelper.helperDialect=mysql
pagehelper.reasonable=true
pagehelper.supportMethodsArguments=true
pagehelper.params=count=countSql

# 启用文件上传
spring.http.multipart.enabled=true 
# 文件大于该阈值时，将写入磁盘，支持KB/MB单位
spring.http.multipart.file-size-threshold=0 
# 自定义临时路径
spring.http.multipart.location=
# 最大文件大小(单个)
spring.http.multipart.maxFileSize=10MB
# 最大请求大小(总体)
spring.http.multipart.maxRequestSize=10MB
file.base.director=/Users/user/Desktop/

# 连接kafka的地址，多个地址用逗号分隔
spring.kafka.bootstrap-servers=47.101.172.249:9092

# 指定默认消费者group id --> 由于在kafka中，同一组中的consumer不会读取到同一个消息，依靠groud.id设置组名
spring.kafka.consumer.group-id=myGroup
# smallest和largest才有效，如果smallest重新0开始读取，如果是largest从logfile的offset读取。一般情况下我们都是设置smallest
spring.kafka.consumer.auto-offset-reset=earliest
spring.kafka.consumer.enable-auto-commit=true
spring.kafka.consumer.auto-commit-interval=100ms
spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer
spring.kafka.consumer.value-deserializer=org.apache.kafka.common.serialization.StringDeserializer


# 写入失败时，重试次数。当leader节点失效，一个repli节点会替代成为leader节点，此时可能出现写入失败，
# 当retris为0时，produce不会重复。retirs重发，此时repli节点完全成为leader节点，不会产生消息丢失。
spring.kafka.producer.retries=1
# 每次批量发送消息的数量,produce积累到一定数据，一次发送
spring.kafka.producer.batch-size=10240
# produce积累数据一次发送，缓存大小达到buffer.memory就发送数据
spring.kafka.producer.buffer-memory=102400
# 关键字的序列化类
spring.kafka.producer.key-serializer = org.apache.kafka.common.serialization.StringSerializer 
# 值的序列化类
spring.kafka.producer.value-serializer = org.apache.kafka.common.serialization.StringSerializer 

# dev
enable.debug = true
spring.devtools.restart.enabled=true
mybatis.configuration.log-impl=org.apache.ibatis.logging.stdout.StdOutImpl


